# ğŸŒ¾ KCC Query Assistant

**KCC Query Assistant** is an AI-powered tool built using Retrieval-Augmented Generation (RAG) to help Indian farmers get accurate and informative responses to agricultural queries. It leverages historical Kisan Call Center (KCC) data and can fall back to live web search when needed.

## ğŸ“Œ Features

- âœ… Query Indian agricultural data using natural language
- ğŸ§  Uses local LLM via Ollama for offline response generation
- ğŸ” Retrieval-Augmented Generation using ChromaDB vector store
- ğŸ”„ Fallback to SerpAPI for web search when no relevant data is found
- ğŸ–¥ï¸ Streamlit web app for easy interaction
- ğŸ› ï¸ Data preprocessing, QA formatting, embedding, and persistent vector store setup

---

## ğŸ› ï¸ Installation

1. **Clone the Repository**
   ```bash
   git clone https://github.com/your-username/kcc-query-assistant.git
   cd kcc-query-assistant
   ```

2. **Create Virtual Environment**
   ```bash
   python -m venv venv
   source venv/bin/activate     # On Windows: venv\Scripts\activate
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ“¦ Dependencies

Here are the key libraries used:

- `streamlit` - UI for the app
- `sentence-transformers` - For generating text embeddings
- `chromadb` - Vector store for document retrieval
- `scipy`, `pandas`, `requests` - Utility & data processing
- `ollama` - Interface with local LLM (`gemma3:1b`)
- `serpapi` - Fallback for web search

Your `requirements.txt` includes all needed packages.

---

## ğŸš€ Quickstart: Launch the App

1. **Ensure Ollama is running**
   - Install [Ollama](https://ollama.com/)
   - Run the model:  
     ```bash
     ollama run gemma3:1b
     ```

2. **Run the Streamlit app**
   ```bash
   streamlit run app.py
   ```

3. **Interact via Web UI**
   - Ask your agricultural queries
   - Get answers from local dataset or SerpAPI

---

## ğŸ§© Project Structure

```bash
kcc-query-assistant/
â”‚
â”œâ”€â”€ app.py                          # Streamlit frontend
â”œâ”€â”€ main.py                         # CLI utility to process & index data
â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚
â”œâ”€â”€ data/                           # Processed CSV/JSON files
â”œâ”€â”€ modules/                        # Modularized project code
â”‚   â”œâ”€â”€ data_preprocessing.py       # Cleaning raw data
â”‚   â”œâ”€â”€ data_transformation.py      # Creating QA pairs
â”‚   â”œâ”€â”€ embedding_generation.py     # Generate text embeddings
â”‚   â”œâ”€â”€ vector_store.py             # ChromaDB collection handling
â”‚   â”œâ”€â”€ rag_pipeline.py             # LLM querying and RAG logic
â”‚   â””â”€â”€ live_search.py              # SerpAPI integration (optional)
â”‚
â””â”€â”€ chroma_store/                   # Persistent vector store
```

---

## âš™ï¸ Offline & Online Answering Logic

- **If relevant context exists in KCC embeddings** â†’ Answer using local LLM (`gemma3:1b`)
- **If no relevant context is found** â†’ Fallback to web search via [SerpAPI](https://serpapi.com/)
- Real-time streaming of LLM responses using the Ollama `stream=True` API

---

## ğŸ§ª Example Prompt

> ğŸ’¬ â€œWhat are common pests in sugarcane crops in Uttar Pradesh during monsoon?â€

---

## ğŸ‘¨â€ğŸŒ¾ Prompt Customization (Used Internally)

```text
You are the Kisan Call Center (KCC) Query Assistant, an expert AI specialized in agricultural advice for Indian farmers. 
You provide clear, accurate, and helpful responses about crops, pests, weather, farming techniques, and related queries based on the KCC dataset. 
Always use any provided context to ground your answers. If no context is available, answer based on your general knowledge in agriculture. 
Be polite, concise, and informative.
```

---

## ğŸ§¼ Preprocessing Details

- Lowercasing and trimming whitespace from important fields: `Crop`, `DistrictName`, `StateName`, `Sector`, etc.
- Conversion of QA pairs from dataset rows
- Embedding each QA pair with `all-MiniLM-L6-v2`
- Storing in persistent ChromaDB collection

---

## ğŸ§  Parameters

You can tune the LLM response style using:

- `temperature = 1` â€“ Higher creativity
- `top_p = 0.8` â€“ Nucleus sampling (diverse choices)

---

## ğŸ”’ API Keys

To use **SerpAPI**, export your API key:

```bash
export SERPAPI_API_KEY="your_key_here"
```

---

## ğŸ“¬ Feedback & Contribution

Pull requests, issues, and feedback are welcome!

---

## ğŸ“œ License

MIT License â€“ Free to use and modify.

